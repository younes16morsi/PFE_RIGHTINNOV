{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import json\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_json('../input/dataset_fr_train.json', lines=True)\n",
    "\n",
    "test_data = pd.read_json('../input/dataset_fr_test.json', lines=True)\n",
    "\n",
    "valid_data= pd.read_json('../input/dataset_fr_dev.json', lines=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert starts into sentiments\n",
    "\n",
    "sentiments_dict = {1: 0,\n",
    "            2: 0,\n",
    "            3: 1,\n",
    "            4: 2,\n",
    "            5: 2}\n",
    "\n",
    "def stars_to_sentiment(dataset):\n",
    "    dataset['sentiment'] = dataset['stars'].map(sentiments_dict)\n",
    "    dataset = dataset[['review_body','sentiment']]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = stars_to_sentiment(train_data)\n",
    "df_test = stars_to_sentiment(test_data)\n",
    "df_valid = stars_to_sentiment(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.reset()\n",
    "            self.strict = False\n",
    "            self.convert_charrefs= True\n",
    "            self.text = StringIO()\n",
    "        def handle_data(self, d):\n",
    "            self.text.write(d)\n",
    "        def get_data(self):\n",
    "            return self.text.getvalue()\n",
    "        \n",
    "def html_free_text(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def url_free_text(text):\n",
    "    text = re.sub(r'(?:\\@|https?\\://)\\S+', '', text)\n",
    "    return text\n",
    "    \n",
    "with open('../input/abrivot_fr.json', encoding='utf-8') as f:\n",
    "        abrivot = json.load(f)   \n",
    "        \n",
    "def abrivot_free_text(text):\n",
    "    words = text.lower().split()\n",
    "    text_out = [abrivot[word] if word in abrivot else word for word in words]\n",
    "    return ' '.join(text_out)\n",
    "\n",
    "def punct_free_text(text):\n",
    "    text_out = simple_preprocess(text, deacc=True, min_len=3)\n",
    "    return ' '.join(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data['text_free_html'] = data['review_body'].apply(lambda x: html_free_text(str(x)))\n",
    "    data['text_free_url'] = data['text_free_html'].apply(url_free_text)\n",
    "    data['text_free_abrivot'] = data['text_free_url'].apply(abrivot_free_text)\n",
    "    data['text_review'] = data['text_free_abrivot'].apply(punct_free_text)\n",
    "    \n",
    "    data = data[['text_review','sentiment']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_train_clean = clean_data(df_train)\n",
    "df_test_clean = clean_data(df_test)\n",
    "df_valid_clean = clean_data(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('fr_core_news_lg', disable=['parser', 'ner'])\n",
    "\n",
    "import fr_core_news_lg\n",
    "nlp = fr_core_news_lg.load(disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/fr_stopwords.txt', encoding='utf-8') as f:\n",
    "    fr_stopwords = f.read().splitlines()\n",
    "stop_words = nlp.Defaults.stop_words.union(fr_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final preprocesser\n",
    "def process_words(texts, stop_words=stop_words):\n",
    "    \n",
    "    \"\"\"Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\"\"\"\n",
    "    \n",
    "    # remove stopwords, short tokens and letter accents \n",
    "    #texts = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts]\n",
    "    texts = [[word for word in doc.split() if word not in stop_words] for doc in texts]\n",
    "\n",
    "    \n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    \n",
    "    # remove stopwords and short tokens again after lemmatization\n",
    "    texts_out = [' '.join(\n",
    "                [word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words]) for doc in texts_out]    \n",
    "    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \n",
    "    data['review_processed'] = process_words(data['text_review'])\n",
    "    data = data[['review_processed','sentiment']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train_proc = process_data(df_train_clean)\n",
    "df_test_proc = process_data(df_test_clean)\n",
    "df_valid_proc = process_data(df_valid_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for further use\n",
    "\n",
    "df_train_proc.to_pickle('df_train_proc.pkl')\n",
    "df_test_proc.to_pickle('df_test_proc.pkl')\n",
    "df_valid_proc.to_pickle('df_valid_proc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = df_train_proc.loc[df_train_proc['sentiment'] == 0, 'review_processed'][:8]\n",
    "positives = df_train_proc.loc[df_train_proc['sentiment'] == 2, 'review_processed'][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = negatives.values.tolist() + positives.values.tolist()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = array([0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3593945a1252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.util.download_model('fr', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('cc.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pfe-env)",
   "language": "python",
   "name": "pfe-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
