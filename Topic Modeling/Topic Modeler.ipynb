{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.58 seconds)\n",
      "Writing emoji data to /home/azureuser/.demoji/codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "import pickle\n",
    "from gensim.models import LdaMulticore, CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lire les données\n",
    "df = pd.read_json('../input/iphone-12-tweets-fr.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtrerer les données\n",
    "# remove duplicates \n",
    "# 840 tweets had been droped\n",
    "\n",
    "df = df[df['lang'] == 'fr']\n",
    "df = df.sort_values(\"content\") \n",
    "  \n",
    "# dropping ALL duplicte values \n",
    "df = df.drop_duplicates(subset =\"content\", keep = 'first')\n",
    "\n",
    "# selectionner que les tweets qui répondent au requetes de l'utilisateur\n",
    "\n",
    "data = df['content']\n",
    "\n",
    "# modifier ici ########\n",
    "with open('../input/keywords.txt', encoding='utf-8') as f:\n",
    "    keywords = f.read().splitlines()\n",
    "\n",
    "data = data[data.str.contains('|'.join(keywords), case=False)]\n",
    "#data = data[data.str.contains('batterie', case=False)]\n",
    "\n",
    "# supprimer les tweets inutiles (publicité, concours ..)\n",
    "\n",
    "with open('../input/ads_words.txt', encoding='utf-8') as f:\n",
    "    ads_words = f.read().splitlines()\n",
    "\n",
    "data = data[~data.str.contains('|'.join(ads_words), case=False)]\n",
    "\n",
    "# modifier ici ########\n",
    "#sauvgarder les données filtrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nettoyer les données\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.reset()\n",
    "            self.strict = False\n",
    "            self.convert_charrefs= True\n",
    "            self.text = StringIO()\n",
    "        def handle_data(self, d):\n",
    "            self.text.write(d)\n",
    "        def get_data(self):\n",
    "            return self.text.getvalue()\n",
    "        \n",
    "def html_free_text(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def emoji_free_text(text):\n",
    "    return demoji.replace(text, '').strip()\n",
    "\n",
    "def url_free_text(text):\n",
    "    text = re.sub(r'(?:\\@|https?\\://)\\S+', '', text)\n",
    "    return text\n",
    "    \n",
    "with open('../input/abrivot_fr.json', encoding='utf-8') as f:\n",
    "        abrivot = json.load(f)   \n",
    "        \n",
    "def abrivot_free_text(text):\n",
    "    words = text.lower().split()\n",
    "    text_out = [abrivot[word] if word in abrivot else word for word in words]\n",
    "    return ' '.join(text_out)\n",
    "\n",
    "def punct_free_text(text):\n",
    "    text_out = simple_preprocess(text, deacc=True, min_len=3)\n",
    "    #return ' '.join(text_out)\n",
    "    return text_out\n",
    "\n",
    "data_free_html = data.apply(html_free_text)\n",
    "data_free_emoji = data_free_html.apply(emoji_free_text)\n",
    "data_free_url = data_free_emoji.apply(url_free_text)\n",
    "data_free_abrivot = data_free_url.apply(abrivot_free_text)\n",
    "data_free_punct = data_free_url.apply(punct_free_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data\n",
    "\n",
    "data_free_punct.to_pickle('../saved models/data_free_punct.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "# Build the bigram and trigrams\n",
    "\n",
    "data = list(data_free_punct)\n",
    "\n",
    "bigram = gensim.models.Phrases(data, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# define a preprocessing function\n",
    "\n",
    "# only need tagger, no need for parser and named entity recognizer, for faster implementation\n",
    "nlp = spacy.load('fr_core_news_lg', disable=['parser', 'ner'])\n",
    "\n",
    "# get stopwords\n",
    "# ask the user to specify the brand name to be added to the stopwords\n",
    "# or implement it manually\n",
    "with open('../input/fr_stopwords.txt', encoding='utf-8') as f:\n",
    "    fr_stopwords = f.read().splitlines()\n",
    "stop_words = nlp.Defaults.stop_words.union(fr_stopwords)\n",
    "\n",
    "\n",
    "# final preprocesser\n",
    "def process_words(texts, stop_words=stop_words, allowed_tags=['NOUN']):\n",
    "    \n",
    "    \"\"\"Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\"\"\"\n",
    "    \n",
    "    # remove stopwords, short tokens and letter accents \n",
    "    #texts = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts]\n",
    "    texts = [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "    \n",
    "    # bi-gram and tri-gram implementation\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    \n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_tags])\n",
    "    \n",
    "    # remove stopwords and short tokens again after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts_out]    \n",
    "    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the corpus and the dictionnary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "dict_corpus = {}\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    for idx, freq in corpus[i]:\n",
    "        if id2word[idx] in dict_corpus:\n",
    "            dict_corpus[id2word[idx]] += freq\n",
    "        else:\n",
    "            dict_corpus[id2word[idx]] = freq\n",
    "\n",
    "dict_df = pd.DataFrame.from_dict(dict_corpus, orient='index', columns=['freq'])\n",
    "\n",
    "threshold = dict_df.sort_values('freq', ascending=False).iloc[9].values[0]\n",
    "extension = dict_df[dict_df.freq>threshold].index.tolist()\n",
    "\n",
    "extension = [word for word in extension if word not in keywords]\n",
    "\n",
    "# add high frequency words to stop words list\n",
    "stop_words.update(extension)\n",
    "# rerun the process_words function\n",
    "data_ready = process_words(data)\n",
    "# recreate Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "id2word.filter_extremes(no_below=20, no_above=0.9)\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dic\n",
    "with open('../saved models/id2word.pkl', 'wb') as fp:\n",
    "        pickle.dump(id2word, fp, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# save the corpus\n",
    "with open(\"../saved models/corpus.txt\", \"wb\") as fp:\n",
    "    pickle.dump(corpus, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "num_topics = list(range(2, 30, 2)[1:])\n",
    "num_keywords = 10\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in num_topics:\n",
    "    LDA_models[i] = LdaMulticore(corpus=corpus,\n",
    "                                id2word=id2word,\n",
    "                                num_topics=i,\n",
    "                                chunksize=2000,\n",
    "                                passes=25,\n",
    "                                iterations=70,\n",
    "                                decay=0.5,\n",
    "                                random_state=100\n",
    "                                )\n",
    "\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=num_keywords,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a function to derive the Jaccard similarity of two topics:\n",
    "\n",
    "def jaccard_similarity(topic_1, topic_2):\n",
    "\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above to derive the mean stability across topics by considering the next topic:\n",
    "\n",
    "LDA_stability = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "        \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the coherence value with the built in gensim\n",
    "\n",
    "coherences = [CoherenceModel(model=LDA_models[i], texts=data_ready,\n",
    "                             dictionary=id2word, coherence='c_v', topn=num_keywords).get_coherence() for i in num_topics[:-1]]\n",
    "\n",
    "# From here derive the ideal number of topics roughly through the difference between the coherence and stability per number of topics:\n",
    "\n",
    "\n",
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(len(num_topics) - 1)[:-1]]\n",
    "#coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "\n",
    "ldamodel = LDA_models[ideal_topic_num]\n",
    "pickle.dump(ldamodel, open(\"../saved models/ldamodel.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pfe-env)",
   "language": "python",
   "name": "pfe-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
