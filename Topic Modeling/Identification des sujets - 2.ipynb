{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook: https://github.com/some-labs-24/data-science/blob/master/python_notebooks/SoMe_NLP_Topic_Modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emoji --upgrade\n",
    "# !pip install pandas-profiling==2.*\n",
    "# !pip install plotly==4.*\n",
    "# !pip install pyldavis\n",
    "# !pip install gensim\n",
    "# !pip install chart_studio\n",
    "# !pip install --upgrade autopep8\n",
    "\n",
    "# # !pip install -U pip setuptools wheel\n",
    "# # !pip install -U spacy\n",
    "# # !python -m spacy download fr_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "\n",
    "#Base and Cleaning \n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import regex\n",
    "import re\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#Natural Language Processing (NLP)\n",
    "import spacy\n",
    "import gensim\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "#from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "#from wordcloud import STOPWORDS\n",
    "#stopwords = set(STOPWORDS)\n",
    "\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading emoji data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... OK (Got response in 0.40 seconds)\n",
      "Writing emoji data to /home/azureuser/.demoji/codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "#!pip install demoji\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "tweets_df = pd.read_json('../input/iphone-12-tweets-fr.json', lines=True)\n",
    "# garder que les colonnes importantes\n",
    "cols = ['date', 'content', 'lang']\n",
    "tweets_df = tweets_df[cols]\n",
    "# pour etre sur que tout les tweets sont en français\n",
    "tweets_df = tweets_df[tweets_df['lang'] == 'fr']\n",
    "\n",
    "tweets_df = tweets_df.sort_values(\"content\") \n",
    "  \n",
    "# dropping ALL duplicte values \n",
    "tweets_df = tweets_df.drop_duplicates(subset =\"content\", keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# selectionner que les tweets qui répondent au requetes de l'utilisateur\n",
    "\n",
    "data = tweets_df['content']\n",
    "\n",
    "keywords = ['stockage', 'écran', 'ecran', 'autonomie', 'réseau', 'reseau', 'alimentation',\n",
    "           'appareil', 'appareil photo', 'processeur', 'multimédia', 'multimedia', 'résolution', 'resolution',\n",
    "           'batterie', 'os', 'pièces', 'pieces' , 'couleur',\n",
    "           'communication', 'sans fil', 'synchronisation', 'coloris', 'poids', 'système', 'systeme',\n",
    "           'dimensions', 'prix', 'service', 'client', 'support', 'payement']\n",
    "\n",
    "data = data[data.str.contains('|'.join(keywords), case=False)]\n",
    "\n",
    "ads_words = [\n",
    "    '#concours', '#jeuconcours', '#giveaway',\n",
    "    '#gagne' ,'#gangner', '#promo', '#promotion', '#publicité',\n",
    "    '#contest', '#ad', '#pub', '#réduction', '#vote', 'votez', 'vote']\n",
    "\n",
    "data = data[~data.str.contains('|'.join(ads_words), case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.columns = ['original_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    return demoji.replace(text, '').strip()\n",
    "\n",
    "def url_free_text(text):\n",
    "    '''\n",
    "    Cleans text from urls\n",
    "    '''\n",
    "    text = re.sub(r'(?:\\@|https?\\://)\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# fonction pour supprimer les symbols HTML\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.reset()\n",
    "            self.strict = False\n",
    "            self.convert_charrefs= True\n",
    "            self.text = StringIO()\n",
    "        def handle_data(self, d):\n",
    "            self.text.write(d)\n",
    "        def get_data(self):\n",
    "            return self.text.getvalue()\n",
    "        \n",
    "def strip_tags(html):\n",
    "    '''\n",
    "    input, output: string\n",
    "    '''\n",
    "    \n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "    \n",
    "def replace_abrivot(text):\n",
    "    '''\n",
    "    input: string\n",
    "    output: string\n",
    "    '''\n",
    "    # remplacement des abréviations\n",
    "\n",
    "    with open('../input/abrivot_fr.json', encoding='utf-8') as f:\n",
    "        abrivot = json.load(f)\n",
    "    words = text.lower().split()\n",
    "    text_out = [abrivot[word] if word in abrivot else word for word in words]\n",
    "    return ' '.join(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pfe-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
    "df['emoji_free_tweets'] = df['original_tweets'].apply(give_emoji_free_text)\n",
    "\n",
    "# Create a new column with url free tweets\n",
    "df['html_free_tweets'] = df['emoji_free_tweets'].apply(strip_tags)\n",
    "\n",
    "# Create a new column with url free tweets\n",
    "df['url_free_tweets'] = df['html_free_tweets'].apply(url_free_text)\n",
    "\n",
    "df['abrivots_free_tweets'] = df['url_free_tweets'].apply(replace_abrivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "# Make sure to restart the runtime after running installations and libraries tab\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mettre à jour la liste d'aprés le fichies des mots vides\n",
    "with open('../input/fr_stopwords.txt', encoding='utf-8') as f:\n",
    "    fr_stopwords = f.read().splitlines()\n",
    "    \n",
    "# Custom stopwords\n",
    "custom_stopwords = ['\\n','\\n\\n', '&amp;', ' ', '.', '-','$', '@']\n",
    "\n",
    "# Customize stop words by adding to the default list\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(fr_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(df['abrivots_free_tweets'], batch_size=500):\n",
    "    doc_tokens = []    \n",
    "    for token in doc: \n",
    "        if token.text.lower() not in ALL_STOP_WORDS:\n",
    "            doc_tokens.append(token.text.lower())   \n",
    "    tokens.append(doc_tokens)\n",
    "\n",
    "# Makes tokens column\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokens a string again\n",
    "df['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df['tokens']]\n",
    "\n",
    "def get_lemmas(text):\n",
    "    '''Used to lemmatize the processed tweets'''\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "df['lemmas'] = df['tokens_back_to_text'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lemmas a string again\n",
    "df['lemmas_back_to_text'] = [' '.join(map(str, l)) for l in df['lemmas']]\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Parses a string into a list of semantic units (words)\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "    Returns:\n",
    "        list: tokens parsed out\n",
    "    \"\"\"\n",
    "    # Removing url's\n",
    "    pattern = r\"http\\S+\"\n",
    "    \n",
    "    tokens = re.sub(pattern, \"\", text) # https://www.youtube.com/watch?v=O2onA4r5UaY\n",
    "    tokens = re.sub('[^a-zA-Z 0-9]', '', text)\n",
    "    tokens = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
    "    tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
    "    tokens = re.sub('@*!*\\$*', '', text) # Remove @ ! $\n",
    "    tokens = tokens.strip(',') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('?') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('!') # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\"'\") # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\".\") # TESTING THIS LINE\n",
    "\n",
    "    tokens = tokens.lower().split() # Make text lowercase and split it\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenizer\n",
    "df['lemma_tokens'] = df['lemmas_back_to_text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a id2word dictionary\n",
    "id2word = Dictionary(df['lemma_tokens'])\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Extremes\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus object \n",
    "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a Base LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=7, id2word=id2word, workers=12, passes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Topic Distance Visualization \n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(base_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypermarameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "data_vectorized = vectorizer.fit_transform(df['lemmas_back_to_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "             estimator=LatentDirichletAllocation(batch_size=128, \n",
    "                                                 doc_topic_prior=None,\n",
    "                                                 evaluate_every=-1, \n",
    "                                                 learning_decay=0.7, \n",
    "                                                 learning_method=None,\n",
    "                                                 learning_offset=10.0, \n",
    "                                                 max_doc_update_iter=100, \n",
    "                                                 max_iter=10,\n",
    "                                                 mean_change_tol=0.001, \n",
    "                                                 n_components=10, \n",
    "                                                 n_jobs=1,\n",
    "                                                 perp_tol=0.1, \n",
    "                                                 random_state=None,\n",
    "                                                 topic_word_prior=None, \n",
    "                                                 total_samples=1000000.0, \n",
    "                                                 verbose=0),\n",
    "             iid=True, n_jobs=1,\n",
    "             param_grid={'n_topics': [10, 15, 20, 30], \n",
    "                         'learning_decay': [0.5, 0.7, 0.9]},\n",
    "             pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "             scoring=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to loop over number of topics to be used to find an \n",
    "#optimal number of tipics\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the \n",
    "    LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values_topic = []\n",
    "    model_list_topic = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list_topic.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values_topic.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list_topic, coherence_values_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,\n",
    "                                                        corpus=corpus,\n",
    "                                                        texts=df['lemma_tokens'],\n",
    "                                                        start=2, limit=200, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_2 = LdaMulticore(corpus=corpus,\n",
    "                       id2word=id2word,\n",
    "                       num_topics=68,\n",
    "                       random_state=42,\n",
    "                       chunksize=2000,\n",
    "                       passes=25,\n",
    "                       decay=0.5,\n",
    "                       iterations=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = model_5_2.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=model_5_2, texts=df['lemma_tokens'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Topic Distance Visualization \n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(model_5_2, corpus, id2word)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python (pfe-env)",
   "language": "python",
   "name": "pfe-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
