{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide jupyter warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../input/iphone-12-tweets-fr.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates \n",
    "# 840 tweets had been droped\n",
    "\n",
    "df = df[df['lang'] == 'fr']\n",
    "df = df.sort_values(\"content\") \n",
    "  \n",
    "# dropping ALL duplicte values \n",
    "df = df.drop_duplicates(subset =\"content\", keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectionner que les tweets qui répondent au requetes de l'utilisateur\n",
    "\n",
    "data = df['content']\n",
    "\n",
    "with open('../input/keywords.txt', encoding='utf-8') as f:\n",
    "    keywords = f.read().splitlines()\n",
    "\n",
    "#data = data[data.str.contains('|'.join(keywords), case=False)]\n",
    "data = data[data.str.contains('batterie', case=False)]\n",
    "\n",
    "# supprimer les tweets inutiles (publicité, concours ..)\n",
    "\n",
    "with open('../input/ads_words.txt', encoding='utf-8') as f:\n",
    "    ads_words = f.read().splitlines()\n",
    "\n",
    "data = data[~data.str.contains('|'.join(ads_words), case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/keywords.txt', encoding='utf-8') as f:\n",
    "    keywords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.reset()\n",
    "            self.strict = False\n",
    "            self.convert_charrefs= True\n",
    "            self.text = StringIO()\n",
    "        def handle_data(self, d):\n",
    "            self.text.write(d)\n",
    "        def get_data(self):\n",
    "            return self.text.getvalue()\n",
    "        \n",
    "def html_free_text(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def emoji_free_text(text):\n",
    "    return demoji.replace(text, '').strip()\n",
    "\n",
    "def url_free_text(text):\n",
    "    text = re.sub(r'(?:\\@|https?\\://)\\S+', '', text)\n",
    "    return text\n",
    "    \n",
    "with open('../input/abrivot_fr.json', encoding='utf-8') as f:\n",
    "        abrivot = json.load(f)   \n",
    "        \n",
    "def abrivot_free_text(text):\n",
    "    words = text.lower().split()\n",
    "    text_out = [abrivot[word] if word in abrivot else word for word in words]\n",
    "    return ' '.join(text_out)\n",
    "\n",
    "def punct_free_text(text):\n",
    "    text_out = simple_preprocess(text, deacc=True, min_len=3)\n",
    "    #return ' '.join(text_out)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_free_html = data.apply(html_free_text)\n",
    "data_free_emoji = data_free_html.apply(emoji_free_text)\n",
    "data_free_url = data_free_emoji.apply(url_free_text)\n",
    "data_free_abrivot = data_free_url.apply(abrivot_free_text)\n",
    "data_free_punct = data_free_url.apply(punct_free_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigrams\n",
    "\n",
    "data = list(data_free_punct)\n",
    "\n",
    "bigram = gensim.models.Phrases(data, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a preprocessing function\n",
    "\n",
    "# only need tagger, no need for parser and named entity recognizer, for faster implementation\n",
    "nlp = spacy.load('fr_core_news_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# get stopwords\n",
    "# ask the user to specify the brand name to be added to the stopwords\n",
    "# or implement it manually\n",
    "with open('../input/fr_stopwords.txt', encoding='utf-8') as f:\n",
    "    fr_stopwords = f.read().splitlines()\n",
    "stop_words = nlp.Defaults.stop_words.union(fr_stopwords)\n",
    "\n",
    "\n",
    "# final preprocesser\n",
    "def process_words(texts, stop_words=stop_words, allowed_tags=['NOUN']):\n",
    "    \n",
    "    \"\"\"Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\"\"\"\n",
    "    \n",
    "    # remove stopwords, short tokens and letter accents \n",
    "    #texts = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts]\n",
    "    texts = [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "    \n",
    "    # bi-gram and tri-gram implementation\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    \n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_tags])\n",
    "    \n",
    "    # remove stopwords and short tokens again after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts_out]    \n",
    "    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_ready = process_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Dictionary\n",
    "\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the corpus\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corpus = {}\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    for idx, freq in corpus[i]:\n",
    "        if id2word[idx] in dict_corpus:\n",
    "            dict_corpus[id2word[idx]] += freq\n",
    "        else:\n",
    "            dict_corpus[id2word[idx]] = freq\n",
    "\n",
    "dict_df = pd.DataFrame.from_dict(dict_corpus, orient='index', columns=['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(dict_df['freq'], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.sort_values('freq', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 3 top words\n",
    "threshold = dict_df.sort_values('freq', ascending=False).iloc[9].values[0]\n",
    "extension = dict_df[dict_df.freq>threshold].index.tolist()\n",
    "\n",
    "extension = [word for word in extension if word not in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask the user to specify the brand name to be added to the stopwords\n",
    "# or implement it manually\n",
    "# tweek the extension list to get words with typo\n",
    "extension.append('appl')\n",
    "extension.append('iphon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add high frequency words to stop words list\n",
    "stop_words.update(extension)\n",
    "# rerun the process_words function\n",
    "data_ready = process_words(data)\n",
    "# recreate Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter extreme words\n",
    "# Filter out words that occur less than 20 documents, or more than\n",
    "# 90% of the documents.\n",
    "\n",
    "id2word.filter_extremes(no_below=20, no_above=0.9)\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore, CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_topics = list(range(1, 50, 1)[1:])\n",
    "num_keywords = 10\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in tqdm(num_topics):\n",
    "    LDA_models[i] = LdaMulticore(corpus=corpus,\n",
    "                                id2word=id2word,\n",
    "                                num_topics=i,\n",
    "                                chunksize=2000,\n",
    "                                passes=25,\n",
    "                                iterations=70,\n",
    "                                decay=0.5,\n",
    "                                random_state=100\n",
    "                                )\n",
    "\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=num_keywords,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a function to derive the Jaccard similarity of two topics:\n",
    "\n",
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above to derive the mean stability across topics by considering the next topic:\n",
    "\n",
    "LDA_stability = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "        \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the coherence value with the built in gensim\n",
    "\n",
    "coherences = [CoherenceModel(model=LDA_models[i], texts=data_ready,\n",
    "                             dictionary=id2word, coherence='c_v', topn=num_keywords).get_coherence() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here derive the ideal number of topics roughly through the difference between the coherence and stability per number of topics:\n",
    "\n",
    "\n",
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(len(num_topics) - 1)[:-1]]\n",
    "#coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finally graph these metrics across the topic numbers:\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
    "ax.set_ylabel('Metric Level', fontsize=20)\n",
    "ax.set_xlabel('Number of Topics', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your ideal number of topics will maximize coherence and minimize the topic overlap based on Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ideal number of topics: ', ideal_topic_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the LDA Model\n",
    "\n",
    "Download the LDA mallet from `!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip`  \n",
    "Extract the model `!unzip mallet-2.0.8.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LDA_models[ideal_topic_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamodel = CoherenceModel(model=ldamodel, texts=data_ready, dictionary=id2word, coherence='c_v', topn=num_keywords)\n",
    "coherence_ldamodel = coherence_model_ldamodel.get_coherence()\n",
    "print('Coherence Score: ', coherence_ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# display topics\n",
    "pprint(ldamodel.show_topics(num_topics = ideal_topic_num, formatted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the model for later\n",
    "\n",
    "import pickle\n",
    "pickle.dump(ldamodel, open(\"../saved models/ldamodel.pkl\", \"wb\"))\n",
    "\n",
    "# load the model\n",
    "#ldamallet = pickle.load(open(\"../saved models/ldamallet.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of topics for each document\n",
    "\n",
    "tm_results = ldamodel[corpus]\n",
    "\n",
    "# We can get the most dominant topic of each document as below:\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] for topics in tm_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most probable words for the given topicis\n",
    "\n",
    "topics = [[(term, round(wt, 3)) for term, wt in ldamodel.show_topic(n, topn=20)] for n in range(0, ldamodel.num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for term-topic matrix:\n",
    "\n",
    "topics_df = pd.DataFrame([[term for term, wt in topic] for topic in topics],\n",
    "                         columns = ['Term'+str(i) for i in range(1, 21)],\n",
    "                         index=['Topic '+str(t) for t in range(1, ldamodel.num_topics+1)]).T\n",
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another option\n",
    "\n",
    "# set column width\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic]) for topic in topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, ldamodel.num_topics+1)] )\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worldcloud\n",
    "\n",
    "# import wordclouds\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# initiate wordcloud object\n",
    "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)\n",
    "\n",
    "# set the figure size\n",
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "\n",
    "# Create subplots for each topic\n",
    "for i in range(8):\n",
    "\n",
    "    wc.generate(text=topics_df[\"Terms per Topic\"][i])\n",
    "    \n",
    "    plt.subplot(5, 4, i+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(topics_df.index[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dominant Topics for Each Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "corpus_topic_df = pd.DataFrame()\n",
    "\n",
    "# get the Titles from the original dataframe\n",
    "corpus_topic_df['Tweet_id'] = data_free_abrivot.index\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Terms'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "corpus_topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pandas groupby function on “Dominant Topic” column and get the document counts for each topic and its percentage in the corpus with chaining agg function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents distribution ovec topics\n",
    "\n",
    "dominant_topic_df = corpus_topic_df.groupby('Dominant Topic').agg(\n",
    "                                  Doc_Count = ('Dominant Topic', np.size),\n",
    "                                  Total_Docs_Perc = ('Dominant Topic', np.size)).reset_index()\n",
    "\n",
    "dominant_topic_df['Total_Docs_Perc'] = dominant_topic_df['Total_Docs_Perc'].apply(lambda row: round((row*100) / len(corpus), 2))\n",
    "\n",
    "dominant_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the most dominant document per topic\n",
    "\n",
    "corpus_topic_df.groupby('Dominant Topic').\\\n",
    "    apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], ascending=False).iloc[0])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minimum_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore, CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values_2(corpus, dictionary, k, mp):\n",
    "    \n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                            id2word=id2word,\n",
    "                            num_topics=k,\n",
    "                            random_state=100,\n",
    "                            chunksize=2000,\n",
    "                            passes=25,\n",
    "                            iterations=70,\n",
    "                            decay=0.5,                            \n",
    "                            #alpha=a,\n",
    "                            #eta=b,\n",
    "                            minimum_probability = mp \n",
    "                            )\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minimum_probabilities = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "results = {}\n",
    "\n",
    "for mp in tqdm(minimum_probabilities):\n",
    "    # get the coherence score for the given parameters\n",
    "    cv = compute_coherence_values_2(corpus=corpus, dictionary=id2word, \n",
    "                                      k=16, mp=mp)\n",
    "    results[mp] = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_probability = list(results.keys())           # list() needed for python 3.x\n",
    "coherence_score = list(results.values())        # ditto\n",
    "plt.plot(minimum_probability, coherence_score, '-')\n",
    "plt.xlabel(\"minimum probability\")\n",
    "plt.ylabel(\"coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                            id2word=id2word,\n",
    "                            num_topics=14,\n",
    "                            random_state=100,\n",
    "                            chunksize=2000,\n",
    "                            passes=25,\n",
    "                            iterations=70,\n",
    "                            minimum_probability=0.6\n",
    "                            )\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "### Dirichlet hyperparameter beta: Word-Topic Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                            id2word=id2word,\n",
    "                            num_topics=ideal_topic_num,\n",
    "                            random_state=100,\n",
    "                            chunksize=2000,\n",
    "                            passes=25,\n",
    "                            iterations=70,\n",
    "                            decay=0.5,                            \n",
    "                            alpha=a,\n",
    "                            eta=b\n",
    "                            )\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v', topn=num_keywords)\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in tqdm(alpha):\n",
    "    # iterare through beta values\n",
    "    for b in tqdm(beta):\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      k=ideal_topic_num, a=a, b=b)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(ideal_topic_num)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(b)\n",
    "        model_results['Coherence'].append(cv)\n",
    "\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "model_results_df.to_csv('../output/lda_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df.iloc[model_results_df['Coherence'].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ideal_alpha = model_results_df.iloc[model_results_df['Coherence'].argmax()].Alpha\n",
    "ideal_beta = model_results_df.iloc[model_results_df['Coherence'].argmax()].Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model EVER!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = LdaMulticore(corpus=corpus,\n",
    "#                             id2word=id2word,\n",
    "#                             num_topics=14,\n",
    "#                             random_state=100,\n",
    "#                             chunksize=2000,\n",
    "#                             passes=25,\n",
    "#                             iterations=70,\n",
    "#                             decay=0.5,                            \n",
    "#                             alpha='symmetric',\n",
    "#                             eta=0.3,\n",
    "#                             )\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                            id2word=id2word,\n",
    "                            num_topics=14,\n",
    "                            random_state=100,\n",
    "                            chunksize=2000,\n",
    "                            passes=25,\n",
    "                            iterations=70,\n",
    "                            decay=0.5,                            \n",
    "                            minimum_probability=0.6\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# display topics\n",
    "pprint(lda_model.show_topics(num_topics = 14, formatted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pfe-env)",
   "language": "python",
   "name": "pfe-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
